[{"categories":["AWS"],"contents":"We will build a continuous delivery pipeline that deploys a React application to Amazon S3 and syncs it with Amazon CloudFront, a content delivery network (CDN) managed by AWS.\nAll source codes are available at https://github.com/minghsu0107/sync-react-s3-cloudfront.\nWhy It is quite convenient to configure a S3 bucket for static website hosting. However, we can only access it via HTTP protocol. A great way to host our contents is to place it to Amazon CloudFront. This way, not only can we benefit from HTTPS protection but also the out-of-the-box caching mechanism CloudFront provides for us.\nSteps Access Policies Your IAM user must be attached with proper S3 access policies and the following CloudFront policies.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Action\u0026#34;: \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:s3:::BUCKET\u0026#34; }, { \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34;, \u0026#34;cloudfront:GetStreamingDistribution\u0026#34;, \u0026#34;cloudfront:GetDistributionConfig\u0026#34;, \u0026#34;cloudfront:GetInvalidation\u0026#34;, \u0026#34;cloudfront:ListInvalidations\u0026#34;, \u0026#34;cloudfront:ListStreamingDistributions\u0026#34;, \u0026#34;cloudfront:ListDistributions\u0026#34; ], \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Where BUCKET is a placeholder of your bucket name.\nS3 Static Website Hosting To enable S3 static website hosting, follow these steps.\n Sign in as your IAM user and open the Amazon S3 console at https://console.aws.amazon.com/s3/. In the Buckets list, choose the name of the bucket that you want to enable static website hosting for. Choose Properties. Under Static website hosting, choose Edit. Choose Use this bucket to host a website. Under Static website hosting, choose Enable. In Index document, enter the file name of the index document, typically index.html. Upload error page. Allow all bucket public access  (optional) Attach bucket policy to grant public read permission on all objects. Or you could grant public read permission during CI flows. We will cover it later.  { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;PublicReadGetObject\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::sendifybucket/*\u0026#34; ] } ] } CloudFront Configuration Next, create a CloudFront web distribution. In addition to the distribution settings that you need for your use case, enter the following:\n For Origin Domain Name, enter your s3 static website endpoint. For example, mybucket.s3-website.us-east-2.amazonaws.com  After the distribution is enabled, visit https://\u0026lt;distribution_id\u0026gt;.cloudfront.net and you will see your website.\nContinuous Delivery We use Drone CI as our automation tool. There are two main steps in the pipeline, which are building React applcation and deploying to AWS.\n Building React application - In this step, we install all dependencies and create a minified bundle to build/ folder.  - name: build-react-app image: node:15.14 commands: - npm install \u0026amp;\u0026amp; npm run build Deploy to AWS - In this step, we upload the minified bundle to S3 and invalidate the CloudFront cache in order to see the updated website instantly.  - name: sync-react-app image: amazon/aws-cli:2.1.29 environment: AWS_ACCESS_KEY_ID: from_secret: aws_access_key_id AWS_SECRET_ACCESS_KEY: from_secret: aws_secret_access_key CLOUDFRONT_DISTRIBUTION_ID: from_secret: cloudfront_distribution_id BUCKET: from_secret: bucket commands: - aws s3 sync ./build/ s3://$BUCKET/ --delete --acl public-read - aws cloudfront create-invalidation --distribution-id $CLOUDFRONT_DISTRIBUTION_ID --paths \u0026#34;/*\u0026#34;  The first command aws s3 sync puts the minify bundle to S3 bucket. The argument --acl public-read grants read permission on our contents to the public. You could ignore this argument if you have enabled public read on all objects via bucket policies mentioned above.\nThe second command aws cloudfront create-invalidation invalidates CloudFront cache in order that the new version becomes visible.\n","permalink":"https://minghsu.io/posts/host-static-website-s3-cloudfront-cd/","tags":["AWS","React","S3","CloudFront"],"title":"[AWS] Host a React Application on S3 and CloudFront with Continuous Delievery"},{"categories":["K8s","Ops"],"contents":"Traefik is a powerful ingress controller with easy deployment and configuration. However, cloud providers like AWS and GCP also provide ingress implementations of their managed Kubernetes.\nIn this post, we will take GCP as example and walk through all needed knowledge of integrating Traefik deployment with ingress provided by GKE. This way, we could enjoy the benefits of feature-rich Traefik CRD as well as convenient infrastruture provisions provided by cloud ingress.\nAll source codes are available at https://github.com/minghsu0107/traefik-ingress-gke.\nTraefik Introduction Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. There are a lot of useful middlewares available, such as regex matching, rate limiting, authentication, and many more. The best part is that Traefik automatically discover new configurations and apply them in real time. For those who are not familiar with Traefik, reading the Traefik documentation is a good start.\nTraefik Configuration In this section, we will cover common configuration of Traefik that is vendor-neutral and compatible with native Kubernetes.\nTraefik CRD Traefik is natively compliant with Kubernetes using the custom resource definitions. Custom resources are extensions of the Kubernetes API, representing a customization of a particular Kubernetes installation. Custom resources help us to declare Traefik components without resorting to lots of ingress annotations.\nTo start with, let\u0026rsquo;s create a namespace traefik:\n--- apiVersion: v1 kind: Namespace metadata: name: traefik Then deploy Traefik CRDs.\n--- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ingressroutes.traefik.containo.us spec: group: traefik.containo.us names: kind: IngressRoute plural: ingressroutes singular: ingressroute scope: Namespaced version: v1alpha1 --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ingressroutetcps.traefik.containo.us spec: group: traefik.containo.us names: kind: IngressRouteTCP plural: ingressroutetcps singular: ingressroutetcp scope: Namespaced version: v1alpha1 --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: ingressrouteudps.traefik.containo.us spec: group: traefik.containo.us names: kind: IngressRouteUDP plural: ingressrouteudps singular: ingressrouteudp scope: Namespaced version: v1alpha1 --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: middlewares.traefik.containo.us spec: group: traefik.containo.us names: kind: Middleware plural: middlewares singular: middleware scope: Namespaced version: v1alpha1 --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: tlsoptions.traefik.containo.us spec: group: traefik.containo.us names: kind: TLSOption plural: tlsoptions singular: tlsoption scope: Namespaced version: v1alpha1 --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: tlsstores.traefik.containo.us spec: group: traefik.containo.us names: kind: TLSStore plural: tlsstores singular: tlsstore scope: Namespaced version: v1alpha1 --- apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: name: traefikservices.traefik.containo.us spec: group: traefik.containo.us names: kind: TraefikService plural: traefikservices singular: traefikservice scope: Namespaced version: v1alpha1 Next, create a service account traefik-ingress-controller and bind a cluster role to it so that Traefik could use the service account to access custom resources.\n--- apiVersion: v1 kind: ServiceAccount metadata: name: traefik-ingress-controller namespace: traefik --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: traefik-ingress-controller rules: - apiGroups: - \u0026#34;\u0026#34; resources: - services - endpoints - secrets verbs: - get - list - watch - apiGroups: - extensions - networking.k8s.io resources: - ingresses - ingressclasses verbs: - get - list - watch - apiGroups: - extensions resources: - ingresses/status verbs: - update - apiGroups: - traefik.containo.us resources: - middlewares - ingressroutes - traefikservices - ingressroutetcps - ingressrouteudps - tlsoptions - tlsstores verbs: - get - list - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: traefik-ingress-controller roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: traefik-ingress-controller subjects: - kind: ServiceAccount name: traefik-ingress-controller namespace: traefik Next, deploy Traefik itself using Kubernetes Deployment type.\n--- apiVersion: apps/v1 kind: Deployment metadata: annotations: prometheus.io/port: http-metrics prometheus.io/scrape: \u0026#34;true\u0026#34; labels: app: traefik name: traefik namespace: traefik spec: replicas: 1 selector: matchLabels: app: traefik template: metadata: labels: app: traefik spec: serviceAccountName: traefik-ingress-controller containers: - name: traefik ports: - containerPort: 80 name: web - containerPort: 8080 name: admin - containerPort: 8082 name: http-metrics args: - --log.level=INFO - --api - --api.dashboard - --api.insecure - --entrypoints.web.address=:80 - --providers.kubernetescrd - --metrics.prometheus.entryPoint=metrics - --entryPoints.metrics.address=:8082 - --accesslog=true - --ping - --ping.entryPoint=web image: traefik:v2.3 readinessProbe: httpGet: path: /ping port: 80 We deploy Traefik in traefik namespace and expose prometheus endpoint at 0.0.0.0:8082/metrics. We also enable readiness probe on 0.0.0.0/ping. This is neccessary because GKE inteprets readiness probe as service health check parameter.\nFinally, deploy a NodePort service to expose Traefik deployment.\n--- apiVersion: v1 kind: Service metadata: annotations: prometheus.io/port: \u0026#34;8082\u0026#34; prometheus.io/scrape: \u0026#34;true\u0026#34; labels: app: traefik name: traefik namespace: traefik spec: type: NodePort ports: - name: web port: 80 protocol: TCP targetPort: 80 - name: http-metrics port: 8082 protocol: TCP targetPort: 8082 selector: app: traefik Deploy to Google Kubernetes Engine (GKE) Instead of directly enabling external load balancer on Traefik service, we handle external traffic by GKE ingress and proxy it to internal Traefik router by HTTP. This way, we could benefit from L7 load balancer features, such as managed certificates and HTTPS redirection, of our ingress instance without modifying any Traefik configuration. The routing now would be like Ingress -\u0026gt; Traefik -\u0026gt; App.\nNote that your GKE cluster needs to be running at least a recent version 1.17-gke for this to work.\nFirst, create a static reserved ip address going by the name of traefik-ip.\ngcloud compute addresses create traefik-ip --global Then, deploy an ingress annotated with managed certificate and HTTPS redirection.\n--- apiVersion: networking.gke.io/v1 kind: ManagedCertificate metadata: name: traefik-cert spec: domains: - mydomain.com - api.mydomain.com --- apiVersion: networking.gke.io/v1beta1 kind: FrontendConfig metadata: name: traefik-frontend-cfg namespace: traefik spec: redirectToHttps: enabled: true responseCodeName: PERMANENT_REDIRECT --- kind: Ingress apiVersion: networking.k8s.io/v1beta1 metadata: name: myingress namespace: traefik annotations: kubernetes.io/ingress.global-static-ip-name: \u0026#34;traefik-ip\u0026#34; networking.gke.io/v1beta1.FrontendConfig: \u0026#34;traefik-frontend-cfg\u0026#34; networking.gke.io/managed-certificates: \u0026#34;traefik-cert\u0026#34; spec: backend: serviceName: traefik servicePort: 80 Above configuration creates a managed certficate on domains mydomain.com and api.mydomain.com. Google Cloud provisions managed certificates valid for 90 days. About one month before expiry, the process to renew your certificate automatically begins. Our ingress instance will use this certificate to handle HTTPS requests. Also, we have set up automatic HTTPS redirection using native GKE resource FrontendConfig.\nConclusion In this post, we have covered details on deploying Traefik to GKE while having it integrated with GKE-native ingress. Such combination helps us decouple Traefik components from cloud ingress features like certificate management and HTTPS redirection, increasing the overall maintainability.\n","permalink":"https://minghsu.io/posts/integrate-traefik-with-cloud-ingress/","tags":["K8s","Traefik","Ingress","GCP","GKE"],"title":"[K8s] All You Need to Know to Integrate Traefik with Cloud Ingress"},{"categories":["AWS"],"contents":"We are going to deploy a sample app container on Amazon ECS using Fargate, a serverless compute engine for containers. Fargate removes the need to provision and manage servers, lets us specify and pay for resources per application, and improves security through application isolation by design.\nSteps First, click get started to create a cluster from the startup template.\nConfigure container definition and task definition. Choose sample-app image and the use the default task definition. This will create a task execution role automatically, which gives right permissions to ECS tasks.\nNext, define the service. A service lets us specify how many copies of task definition to run and maintain in a cluster. We can optionally use an Elastic Load Balancing (ELB) load balancer to distribute incoming traffic to containers in the service. Amazon ECS maintains that number of tasks and coordinates task scheduling with the load balancer. We can also optionally use Service Auto Scaling to adjust the number of tasks in the service. Here, we will use an Application Load Balancer (ALB) to route incoming requests to our sample app.\nFinally, configure your ECS cluster. We could either create a new VPC and subnets automatically or attach an existed VPC to our cluster.\nOverview:\nWatch the ECS cluster lauch status. The creation takes a few minutes.\nAfter the creation is completed, we can see that there is one service and one running task in our cluster. A task is the instantiation of a task definition within a cluster. After you have created a task definition for your application within Amazon ECS, you can specify the number of tasks to run on your cluster.\nCheck the service details. We can see that our task, load balancing, and networking all work successfully.\nClick Tasks tab and check details of our only task. In the Network section, we can see the auto-assign public IP (18.188.40.78) of our container.\nVisit 18.188.40.78 and we will see our sample app running.\nWe could also visit our sample app through our Application Load Balancer. Check DNS name of your ALB in the Load Balancer Dashboard:\nVisit EC2Co-EcsEl-PR126DKAQE76-1958981970.us-east-2.elb.amazonaws.com:\nOptionally, we could enable container auto scaling. By specifying minimum/maximum number of tasks, we are able to adjust number of running tasks dynamically.\nThe startup template creates a ECS cluster using a CloudFormation stack template. Let\u0026rsquo;s see what resources it have created for us.\nA default VPC for ECS is created:\nTwo security groups are created: an Application Load Balancer security group that allows all traffic on the Application Load Balancer port (HTTP) and an Amazon ECS security group that allows all traffic on the HTTP port or all traffic ONLY from the Application Load Balancer security group.\nTwo subnets, PublicSubnetAz1 (AZ: us-east-2a) and PublicSubnetAz2 (AZ: us-east-2b) in the default VPC are created:\nNote that our container is abled to be assigned a public IP because it is in a public subnet, to which an Internet Gatway is attached.\nFinally, we could delete the cluster by clicking Delete on the cluster dashboard. This will delete all resources we have created.\nAppendix - Using Images on ECR An Amazon ECS task execution role is automatically created in the Amazon ECS console first-run experience. Thus, we only need to ensure that our ECS owner is as same as our ECR owner in order to pull images successfully.\nIn Container definition section of the startup template, choose the custom image:\nWe need to specify port mappings of the container so that the Elastic Load Balancing load balancer knows which ports of the container it should send to.\n","permalink":"https://minghsu.io/posts/deploy-docker-containers-on-ecs/","tags":["AWS","Docker","ECS"],"title":"[AWS] Deploy Docker Containers on ECS"},{"categories":["AWS","Golang"],"contents":"Amazon CloudFront is a fast content delivery network (CDN) service managed by AWS. It serves your contents across edge locations around the globe with high transfer speeds and low latency under secured connections.\nIn this post, we will set up an Amazon CloudFront distribution that serves private contents on your S3 bucket in order to speed up your content retrival while fully controlling user access permissions.\nSetting up CloudFront First, create a key pair for later use:\nopenssl genrsa -out cfprikey.pem 2048 openssl rsa -in cfprikey.pem -outform PEM -pubout -out cfpublic.pem Create CloudFront public key using the public key you just created:\nAfter the public key is created, it will be given a public key ID. You will need it when signing CloudFront URLs.\nCreate a key group that is associated with the CloutFront public key mypublickey:\nThen create a CloudFront distribution. Connect its orgin to your S3 bucket mybucket. Also, enable the CloudFront bucket access restriction and add a new CloudFront origin access identity to your S3 permission policy:\nEnable the CloudFront viewer access restriction and connect it to your key group:\nCheck your CloudFront distribution status here. Wait until its status becomes Deployed. Then you will see the domain name of your CloudFront distribution.\nWe enabled the CloudFront bucket access restriction so that clients cannot access your object using S3 URL but only CloudFront URLs.\nWhenever your users access your Amazon S3 objects through CloudFront, the CloudFront origin access identity retrieves the objects on behalf of your users. If your users request objects directly by using Amazon S3 URLs, he or she will be denied. The procedure can be summarized in the following figure:\nIn addition, we restrict viewer access so that only requests through signed URLs are allowed to access your content. The signing mechanism works by hashing and signing one part of the URL using the private key from your public–private key pair. When someone uses a signed URL to access a file, CloudFront compares the signed and unsigned portions of the URL. If they don\u0026rsquo;t match, CloudFront doesn\u0026rsquo;t serve the file.\nIf you don\u0026rsquo;t restrict viewer access, all requests through CloudFront URLs are allowed to access your backend S3 objects, ie. they are public to the entire world. So please be sure to enable the viewer access restriction if you want to protect your private contents.\nSigned URL Creation Example You can see the complete source code example on my Github.\nIn this example, we will use Golang AWS SDK to upload hello.txt to S3 bucket mybucket with key mysubpath/hello.txt. Then we will create its signed URL, which has a 1 hour expiration. Users can only access your private content hello.txt through this signed URL.\nFirst, create a new client session:\ncreds := credentials.NewStaticCredentials(accessKey, secretKey, \u0026#34;\u0026#34;) config := \u0026amp;aws.Config{ Credentials: creds, Region: aws.String(s3Region), MaxRetries: aws.Int(3), } session, err := session.NewSession(config) Upload hello.txt to the S3 bucket:\nfromFile, err := os.Open(uploadFrom) if err != nil { exitErrorf(\u0026#34;Unable to open file %q, %v\u0026#34;, uploadFrom, err) } defer fromFile.Close() uploader := s3manager.NewUploader(session) var output *s3manager.UploadOutput output, err = uploader.UploadWithContext(context.Background(), \u0026amp;s3manager.UploadInput{ Bucket: aws.String(s3Bucket), Key: aws.String(objKey), Body: fromFile, }) Sign the CloudFront object URL of hello.txt with the CloudFront public key ID and private key you created previously:\nvar priKey *rsa.PrivateKey priKey, err = sign.LoadPEMPrivKey(priKeyFile) if err != nil { exitErrorf(\u0026#34;err loading private key, %v\u0026#34;, err) } var signedURL string signer := sign.NewURLSigner(cfAccessKey, priKey) rawURL := url.URL{ Scheme: \u0026#34;https\u0026#34;, Host: cfDomain, Path: objKey, } signedURL, err = signer.Sign(rawURL.String(), time.Now().Add(1*time.Hour)) if err != nil { exitErrorf(\u0026#34;Failed to sign url, err: %v\u0026#34;, err) } fmt.Printf(\u0026#34;Get signed URL %q\\n\u0026#34;, signedURL) The CloudFront object URL https://mycfdomain.cloudfront.net/mysubpath/hello.txt is now signed, and th result is printed in the standard output. Users can access hello.txt through this signed URL until it expires.\nReference  https://medium.com/@ratulbasak93/serving-private-content-of-s3-through-cloudfront-signed-url-593ede788d0d ","permalink":"https://minghsu.io/posts/aws-cloudfront-with-signed-url/","tags":["AWS","S3","CloudFront","Golang"],"title":"[AWS] AWS CloudFront with Signed URL"},{"categories":["Ops","K8s"],"contents":"The following diagram shows a typical high-availibility Kubernetes cluster with embedded etcd:\nHowever, HA architecture also brings about another problem - which K8s API server should the client connect to? How to detect a node failure while fowarding traffic in order to achieve high availibility?\nEach of K8s controller plane replicas will run the following components in the following mode:\n etcd instance: all instances will be clustered together using consensus; API server: each server will talk to local etcd - all API servers in the cluster will be available; controllers, scheduler, and cluster auto-scaler: will use lease mechanism - only one instance of each of them will be active in the cluster; add-on manager: each manager will work independently trying to keep add-ons in sync  But how to determine which K8s API server the client should connect to while ensuring healthiness of the backend servers? Simply adding mulitple DNS A records does not solve the problem since DNS server could not check server healthiness in real time. A reverse proxy with upstream health check is still not good enough because it would become a single-point-of-failure.\nThe concept of virtual ip comes to the rescue.\nTo achieve high availibility, we will use Keepalived, a LVS (Linux Virtual Server) solution based on VRRP protocol. A LVS service contains a master and multiple backup servers while exposing a virtual ip to outside as a whole. The virtual ip will be pointed to the master server. The master server will send heartbeats to backup servers. Once backup servers are not receiving heartbeats, one backup server will take over the virtual ip, ie. the virtual ip will \u0026ldquo;float\u0026rdquo; to that backup server.\nTo sum up, we have the following architecture:\n A floating vip maps to three keepalived servers (one master and two backups). Each keepalived server runs on a K8s controller plane replica. When keepalived master server fails, vip floats to another backup server automatically. HAProxy load-balances traffic to three K8s API servers.  The following section is hands-on tutorial that implements the above architecture.\nSettings  Three controller plane nodes in private network: 172.16.0.0/16 Node private IPs: 172.16.217.171, 172.16.217.172, 172.16.217.173 Virtual IP: 172.16.100.100, 172.16.100.101  Usage You can find full source code on my Github.\nHAProxy configuration for load balancing on K8s API servers (haproxy.cfg):\nglobal log /dev/log local0 warning maxconn 4000 daemon defaults log global option httplog option dontlognull timeout connect 5000 timeout client 50000 timeout server 50000 frontend kube-apiserver bind *:\u0026#34;$HAPROXY_PORT\u0026#34; mode tcp option tcplog default_backend kube-apiserver backend kube-apiserver mode tcp option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server kube-apiserver-1 172.16.217.171:6443 check # simple http health check server kube-apiserver-1 172.16.217.172:6443 check server kube-apiserver-1 172.16.217.173:6443 check Keepalived configuration (keepalived.conf):\nglobal_defs { default_interface {{ KEEPALIVED_INTERFACE }} } vrrp_script chk_haproxy { # check haproxy script \u0026#34;/bin/bash -c \u0026#39;if [[ $(netstat -nlp | grep 7443)]]; then exit 0; else exit 1; fi\u0026#39;\u0026#34; interval 2 # check every two seconds weight 11 } vrrp_instance VI_1 { interface {{ KEEPALIVED_INTERFACE }} state {{ KEEPALIVED_STATE }} virtual_router_id {{ KEEPALIVED_ROUTER_ID }} priority {{ KEEPALIVED_PRIORITY }} advert_int 2 unicast_peer { {{ KEEPALIVED_UNICAST_PEERS }} } virtual_ipaddress { {{ KEEPALIVED_VIRTUAL_IPS }} } authentication { auth_type PASS auth_pass {{ KEEPALIVED_PASSWORD }} } track_script { chk_haproxy } {{ KEEPALIVED_NOTIFY }} } This configure keepalived server to check if HAProxy is healthy every 2 seconds. Some important environment variables:\n KEEPALIVED_INTERFACE: Keepalived network interface. Defaults to eth0 KEEPALIVED_PASSWORD: Keepalived password. Defaults to d0cker KEEPALIVED_PRIORITY Keepalived node priority. Defaults to 150 KEEPALIVED_ROUTER_ID Keepalived virtual router ID. Defaults to 51 KEEPALIVED_UNICAST_PEERS Keepalived unicast peers. Defaults to 192.168.1.10, 192.168.1.11 KEEPALIVED_VIRTUAL_IPS: Keepalived virtual IPs. Defaults to 192.168.1.231, 192.168.1.232 KEEPALIVED_NOTIFY: Script to execute when node state change. Defaults to /container/service/keepalived/assets/notify.sh KEEPALIVED_STATE: The starting state of keepalived; it can either be MASTER or BACKUP.  We will use docker to run HAProxy and on each K8s controller plane replicas.\nOn first node:\ndocker run -d --net=host --volume $(pwd)/config/haproxy:/usr/local/etc/haproxy \\  -e HAPROXY_PORT=7443 \\  haproxy:2.3-alpine docker run -d --cap-add=NET_ADMIN --cap-add=NET_BROADCAST --cap-add=NET_RAW --net=host \\  --volume $(pwd)/config/keepalived/keepalived.conf:/container/service/keepalived/assets/keepalived.conf \\  -e KEEPALIVED_INTERFACE=eth0 \\  -e KEEPALIVED_PASSWORD=pass \\  -e KEEPALIVED_STATE=MASTER \\  -e KEEPALIVED_VIRTUAL_IPS=\u0026#34;#PYTHON2BASH:[\u0026#39;172.16.100.100\u0026#39;, \u0026#39;172.16.100.101\u0026#39;]\u0026#34; \\  -e KEEPALIVED_UNICAST_PEERS=\u0026#34;#PYTHON2BASH:[\u0026#39;172.16.217.171\u0026#39;, \u0026#39;172.16.217.172\u0026#39;, \u0026#39;172.16.217.173\u0026#39;]\u0026#34; \\  osixia/keepalived:2.0.20 --copy-service On second node:\ndocker run -d --net=host --volume $(pwd)/config/haproxy:/usr/local/etc/haproxy \\  -e HAPROXY_PORT=7443 \\  haproxy:2.3-alpine --volume $(pwd)/config/keepalived/keepalived.conf:/container/service/keepalived/assets/keepalived.conf \\ docker run -d --cap-add=NET_ADMIN --cap-add=NET_BROADCAST --cap-add=NET_RAW --net=host \\  -e KEEPALIVED_INTERFACE=eth0 \\  -e KEEPALIVED_PASSWORD=pass \\  -e KEEPALIVED_STATE=BACKUP \\  -e KEEPALIVED_VIRTUAL_IPS=\u0026#34;#PYTHON2BASH:[\u0026#39;172.16.100.100\u0026#39;, \u0026#39;172.16.100.101\u0026#39;]\u0026#34; \\  -e KEEPALIVED_UNICAST_PEERS=\u0026#34;#PYTHON2BASH:[\u0026#39;172.16.217.171\u0026#39;, \u0026#39;172.16.217.172\u0026#39;, \u0026#39;172.16.217.173\u0026#39;]\u0026#34; \\  osixia/keepalived:2.0.20 --copy-service On third node:\ndocker run -d --net=host --volume $(pwd)/config/haproxy:/usr/local/etc/haproxy \\  -e HAPROXY_PORT=7443 \\  haproxy:2.3-alpine docker run -d --cap-add=NET_ADMIN --cap-add=NET_BROADCAST --cap-add=NET_RAW --net=host \\  --volume $(pwd)/config/keepalived/keepalived.conf:/container/service/keepalived/assets/keepalived.conf \\  -e KEEPALIVED_INTERFACE=eth0 \\  -e KEEPALIVED_PASSWORD=pass \\  -e KEEPALIVED_STATE=BACKUP \\  -e KEEPALIVED_VIRTUAL_IPS=\u0026#34;#PYTHON2BASH:[\u0026#39;172.16.100.100\u0026#39;, \u0026#39;172.16.100.101\u0026#39;]\u0026#34; \\  -e KEEPALIVED_UNICAST_PEERS=\u0026#34;#PYTHON2BASH:[\u0026#39;172.16.217.171\u0026#39;, \u0026#39;172.16.217.172\u0026#39;, \u0026#39;172.16.217.173\u0026#39;]\u0026#34; \\  osixia/keepalived:2.0.20 --copy-service Finally, point your dns to either 172.16.100.100 or 172.16.100.101. From now on, a request with url \u0026lt;https://your.domain.com:7443\u0026gt; will be resolved to https://\u0026lt;172.16.100.100|172.16.100.101\u0026gt;:7443, rounted to the current keepalived master, and eventually sent to one of the three K8s API servers by HAProxy.\nReference  https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/ https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/ https://www.kubernetes.org.cn/6964.html https://www.mdeditor.tw/pl/pRgX/zh-tw ","permalink":"https://minghsu.io/posts/k8s-ha/","tags":["K8s","HA","Keepalived"],"title":"[K8s] Learn Kubernetes HA Architecture with Hands-On Example"},{"categories":["Python"],"contents":"在運行程式時，我們常希望善用系統的多核環境平行處理資料以提升計算的效率。然而 Python 的平行化似乎沒有那麼簡單。這篇文章會簡單介紹平行運算的概念、平行化 Python 時會遇到的問題、以及如何解決它們。\nMultiprocessing vs. Multithreading Multiprocessing 是一種將程式或指令同時跑在多個平行 process 的運算模式。當你的工作可以高度平行化時，也就是 subtask 之間是互相獨立的情況下，multiprocessing 就可以利用多核的環境幫助你加速運算。\n說到 multiprocessing 就不得不提到 multithreading。Multithreading 與 multiprocessing 很相似，目標都是要將工作平行化處理，然而在 multithreading 中，多個平行的 thread 共享 heap (動態記憶體空間)、data section (global/staic 變數)、與一些 OS 資源，如 open files 與 signals。因此若多個 thread 共享某塊 memory 內的資料，那麼程式設計者就必須使用一系列 synchronization 的技巧來預防 dead lock 與 race condition。這是一個很大的主題，在這邊先不深究，以後有機會會寫一篇文章另外講解。\nPython 與平行化 CPython 有一個 GIL (Global Interpreter Lock) 的設計，它保證在任何時刻只會有一個 thread 執行 Python code 以確保 multithreading 下的 thread safety。當執行一定數量的 code 或是 I/O 被阻塞時， CPython 為了平衡不同 thread 的執行時間，會強制釋放 GIL 並轉移到其他的 thread 上。\n在 single thread 的情況下 GIL 不會造成嚴重的問題。但是當我們嘗試把工作分佈到多個 thread 上時，GIL 會 CPU-bound 的工作效率低下，這是因為 GIL 限制了多個 thread 的並行。要注意的是，GIL 不會對 IO-bound task 造成影響，這是因為 GIL 在 thread 等待 I/O 時就會被釋放。\n然而在資料處理上，大部分的 task 都還是 CPU-Bound 的，難道 Python 就不能平行話 CPU-bound 的工作了嗎？ 當然不是。我們還有 multiprocessing 可以使用！當我們運行多個 Python 的 process 時，每個 Process 都會有一個它自己的 Python interpreter 與記憶體空間，這解決了上述提到的 GIL 的問題。雖然運行多個 process 的開銷遠比 thread 還要大，但對於 CPU-bound task 來說這樣的 overhead 是可以忽略的。\n接下來會展示如何以 Python 的 multiprocessing 套件實作 worker pool，也就是維護數個運行的 worker，每個 worker 都是一個 process，並將 task 切成數個 subtask 分配給這群 worker。當一個 worker 完成當前工作後，它會去檢查還有沒有尚未被其他 worker 執行的 subtask，若有則拿去執行。\nWorker pool 讓我們能夠節省系統資源的開銷。如果我們每新增一個 subtask 就創建一個 process，不僅 overhead 很大而且也會很快就耗盡系統資源。\n實作 Worker Pool 假設 demo 函式是我們希望平行運行的 subtask，它簡單的將兩個整數相加並印出結果：\ndef demo(a: int, b: int): print(f\u0026#39;{a} + {b} = {a+b}\u0026#39;) return a + b 使用當前 cpu 數量做為 pool size 並行處理 demo(0, 0), demo(1, 1) \u0026hellip; demo(9, 9)。starmap() 方法讓我們傳入需要的參數並阻塞等待所有 subtask 完成，最後在結束時返回所有運行的結果：\nfrom multiprocessing import Pool from multiprocessing import cpu_count def pool_sync(): pool_sz = cpu_count() with Pool(pool_sz) as p: res = p.starmap(demo, [(i, i) for i in range(10)]) p.close() p.join() print(res) 使用 starmap_async 會馬上返回並回傳一個 callback。我們可以用 callback.get() 來取得所有運行結果：\ndef pool_async(): pool_sz = cpu_count() with Pool(pool_sz) as p: # returns immediately cb = p.starmap_async(demo, [(i, i) for i in range(10)]) res = cb.get() p.close() p.join() print(res) 完整程式碼：\nfrom multiprocessing import Pool from multiprocessing import cpu_count def demo(a: int, b: int): print(f\u0026#39;{a} + {b} = {a+b}\u0026#39;) return a + b def pool_sync(): pool_sz = cpu_count() with Pool(pool_sz) as p: res = p.starmap(demo, [(i, i) for i in range(10)]) p.close() p.join() print(res) def pool_async(): pool_sz = cpu_count() with Pool(pool_sz) as p: # returns immediately cb = p.starmap_async(demo, [(i, i) for i in range(10)]) res = cb.get() p.close() p.join() print(res) if __name__ == \u0026#39;__main__\u0026#39;: pool_sync() pool_async() 可能的 output：\n0 + 0 = 0 1 + 1 = 2 2 + 2 = 4 3 + 3 = 6 4 + 4 = 8 5 + 5 = 10 7 + 7 = 14 6 + 6 = 12 8 + 8 = 16 9 + 9 = 18 [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 0 + 0 = 0 1 + 1 = 2 2 + 2 = 4 3 + 3 = 6 5 + 5 = 10 4 + 4 = 8 6 + 6 = 12 9 + 9 = 18 7 + 7 = 14 8 + 8 = 16 [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 注意到每個 subtask 並不是按照順序而是平行的執行，但最後返回的結果是按照我們輸入給 starmap 或是 starmap_async 的參數順序的。\n總結 在資料科學的領域中，我們常常需要從資料庫撈出海量資料、預處理後再做進一步的分析。如果能善用資料平行處理的技巧可以大大提高運算的效率。希望大家看完這篇文章後能更了解平行運算與 GIL 的概念，並且能實際應用在日常的工作中！\n","permalink":"https://minghsu.io/posts/python-parallel/","tags":["Python","Parallel"],"title":"平行化你的 Python 程式"},{"categories":["Web","Golang"],"contents":"When developing HTTP APIs, we may have to process the same request-specific data throughout middlewares. Since it\u0026rsquo;s a quite common pattern, I decide to figure it out and share how I solve it.\nExample Here we are using Gin, a web framework written in golang featuring performance and good productivity.\nSuppose we have an api endpoint /api/hello handled by HelloHandleFunc. For each incoming request, we want to authenticate its JWT (in Authentication header) and pass the decoded user ID to HelloHandleFunc. This website clearly illustrates what JWT is.\nThe main idea is to make use of context.Context in http.Request. The context of each request controls the entire request life cycle. Thus, we can set request-specific data (key-value pairs) in the context using context.WithValue() and retrives it using Context.Value().\nThe following code shows the implementation of AddUserID() middleware, which summarizes the above procedure:\nfunc extractToken(r *http.Request) string { bearToken := r.Header.Get(conf.JWTAuthHeader) strArr := strings.Split(bearToken, \u0026#34; \u0026#34;) if len(strArr) == 2 { return strArr[1] } return \u0026#34;\u0026#34; } func AddUserID() gin.HandlerFunc { return func(c *gin.Context) { accessToken := extractToken(c.Request) if accessToken == \u0026#34;\u0026#34; { c.AbortWithStatus(http.StatusUnauthorized) return } userID := DecodeJWT(accessToken) c.Request = c.Request.WithContext(context.WithValue(c.Request.Context(), \u0026#34;user_id\u0026#34;, userID)) c.Next() } } Retrieve user_id from the context in HelloHandleFunc:\nfunc HelloHandleFunc(c *gin.Context) { customerID, ok := c.Request.Context().Value(\u0026#34;user_id\u0026#34;).(string) if !ok { c.Abort() return } c.String(http.StatusOK, fmt.Printf(\u0026#34;hi, %s!\u0026#34;, customerID)) return } Register the route with AddUserID() middleware enabled:\nrouter := gin.New() group := router.Group(\u0026#34;/api/hello\u0026#34;) group.Use(AddUserID()) group.GET(\u0026#34;/hello\u0026#34;, HelloHandleFunc) Start the server:\nrouter.Run(\u0026#34;:8080\u0026#34;) ","permalink":"https://minghsu.io/posts/http-context/","tags":["Web","Golang","Gin-Gonic","HTTP"],"title":"Passing Parameters in Golang HTTP Context"},{"categories":["Web","Golang"],"contents":"這篇文章統整了我在 Golang Taipei #55 Meetup 分享的內容。\nEvent-driven architecture 在近幾年越來越受關注，它不僅幫助我們解耦服務組件、反轉依賴，更可提高系統的 throughput，大幅提升了擴展性。\n這次主題會講解 Event-driven 的核心概念，簡介幾種常見的分佈式消息系統，並展示如何輕鬆用 Golang 實作 event-driven application，幫助大家能更快理解。\n 什麼是 Event Event 可以是一個「改變系統狀態」的變化，也可以是陳述當前系統狀態的「事實」，如使用者的點擊、sensor 的資料流、一筆成立的訂單資訊等。而產生 event 的一方叫生產者 (producer)，接收 event 的一方叫消費者 (consumer)。\n 如上圖，event publisher 產生了一個 event 後，數個對此事件有興趣的 consumer 都可以訂閱它。\n事件驅動架構 主流的架構有兩種：\n Pub/Sub model  消費者訂閱一到數個事件流，當一個事件產生(被發布)後會被送給有訂閱它的消費者\nEvent streaming  事件被 append 進 log 並存在 event store。不同於 Pub/Sub 模型，Consumer 可以參與任一事件流並從任何一個時間點開始 \u0026ldquo;replay\u0026rdquo;，產生一個 view，這個過程叫做 view generation。相關的應用有 Event sourcing、CQRS (讀寫分離) 等。\n 上圖簡單的描述了一個 event sourcing 的架構。由 end users 產生的各種 event 會被存在 event store，而每個 consumer 各對應到一個 view generation，並寫道 read storage，而外部所有的 query 只會訪問 read storage。這樣讀寫分離的架構讓我們能更有彈性的根據不同場景選擇適合的 DB，比如需要全文檢索的時候就可以考慮使用 ElasticSearch 作為 read storage 等。\nWhy 事件驅動  使用事件來溝通不同服務幫助我們明確建立 Domain event、並且維持程式邊界，進而維持服務自治性。 解耦系統組件、鬆散依賴。 事件的異步處理幫助提高系統的 throughput、提高整體架構的擴展性。 反轉依賴，讓系統更貼近真實業務邏輯關係。 幫助我們建構 responsive system：   事件驅動與微服務  從上圖可以看到，微服務之間藉由 event bus (message broker) 使用事件彼此溝通 (pub/sub)，而每個微服務都各自維護一個 database，並訂閱與自身服務相關的事件。由此可以發現，一個微服務可以是生產者、消費者、或是兩者都是。\n注意事項  使用前需要評估系統對資料一致性的要求：當資料具最終一致性的場境較容易處理 要小心分散式交易時的資料一致性問題：在分散式的架構下，我們不再能使用單個 DB transaction 確保交易的原子性。當一筆交易分佈在多個服務時確保交易一致性的方法：多階段提交、saga pattern。 額外的維運成本：相比單體架構，有更多的服務與外部系統要維護、監控   小結：事件驅動並不是 silver bullet，還是要看應用場景選擇最適合的架構。\n用 Golang 實作事件驅動 Watermill  https://github.com/ThreeDotsLabs/watermill   Watermill 是一個幫助我們實作 message streaming 的 Golang library，它統一 publish/subscribe 介面，因此可以輕鬆換到不同底層 broker 而不需修改核心程式碼。\n另外，它有著充滿彈性的 API ，讓我們可以掌握 broker-specific 的設定，比如使用 Kafka 時可以直接 override Sarama 的 config，完成更細部的客戶端設定。Watermill 同時也提供開箱即用的 middleware，讓我們不用手刻 Timeout、Retry、Recovery 等諸多功能。值得一提的是，除了 Kafka 或 RabbitMQ 等常見的 broker，watermill 也支持 HTTP 或是 MySQL binlog，因此實用性滿高的。\nWatermill 的核心就是 Pub/Sub 的 interface。它將所有種類的 message broker 都封裝成 Publisher 與 Subscriber 的介面，讓我們的程式碼可以與底層的 client library 解耦：\ntype Publisher interface { Publish(topic string, messages ...*Message) error Close() error } type Subscriber interface { Subscribe(ctx context.Context, topic string) (\u0026lt;-chan *Message, error) Close() error } Demo 完整程式碼請看這裡。\n 有一個 publisher 每三秒向 incoming_topic 發布一個新消息 helloHandler 與 incomingTopicHandler 訂閱了 incoming_topic 這個主題，而 outgoingTopicHandler 則訂閱了 outgoing_topic 當 helloHandler 收到了一個新消息，它會再發布另一個 greeting message 到 outgoing_topic 最後訂閱了 outgoing_topic 的 outgoingTopicHandler 收到了這個 greeting message。  接著來看看程式碼實作。首先我們建立一個 router，router 負責管理所有的 pub/sub handler，並且可以在 router 上註冊 global 的 middleware：\nrouter.AddPlugin(plugin.SignalsHandler) router.AddMiddleware( middleware.CorrelationID, middleware.Timeout(time.Second*10), middleware.NewThrottle(10, time.Second).Middleware, middleware.Retry{ MaxRetries: 5, Logger: logger, }.Middleware, middleware.Recoverer, ) 這邊展示了一些常用的 middleware，比如 Timeout、Throttle、Retry 與 Recovery 機制等。\n由於我們使用 NATS Streaming 作為底層的 broker，因此我們可以寫一個 NATS Streaming client 的 factory，它會回傳前面所提到的 Publisher 或是 Subscriber：\nfunc NewNATSPublisher(logger watermill.LoggerAdapter, clusterID, natsURL string) (message.Publisher, error) { return nats.NewStreamingPublisher( nats.StreamingPublisherConfig{ ClusterID: clusterID, ClientID: watermill.NewShortUUID(), StanOptions: []stan.Option{ stan.NatsURL(natsURL), }, Marshaler: marshaler, }, logger, ) } func NewNATSSubscriber(logger watermill.LoggerAdapter, clusterID, clientID, natsURL string) (message.Subscriber, error) { return nats.NewStreamingSubscriber( nats.StreamingSubscriberConfig{ ClusterID: clusterID, ClientID: clientID, StanOptions: []stan.Option{ stan.NatsURL(\u0026#34;nats://nats-streaming:4222\u0026#34;), }, Unmarshaler: marshaler, }, logger, ) } 接著我們向 router 註冊 helloHandler 與只有作用在 helloHandler 的 middleware：\nhandler := router.AddHandler( \u0026#34;hello_handler\u0026#34;, incomingTopic, subscriber, outgoingTopic, publisher, helloHandler{}.Handler, ) handler.AddMiddleware(func(h message.HandlerFunc) message.HandlerFunc { return func(message *message.Message) ([]*message.Message, error) { fmt.Printf(\u0026#34;\\nexecuting hello_handler specific middleware for %s\u0026#34;, message.UUID) return h(message) } }) 註冊 incomingTopicHandler 與 outgoingTopicHandler：\nrouter.AddNoPublisherHandler( incomingTopic+\u0026#34;_handler\u0026#34;, incomingTopic, subscriber, incomingTopicHandler{}.HandlerWithoutPublish, ) router.AddNoPublisherHandler( outgoingTopic+\u0026#34;_handler\u0026#34;, outgoingTopic, subscriber, outgoingTopicHandler{}.HandlerWithoutPublish, ) 最後在背景每三秒向 incomingTopic 發布訊息，同時啟動 router：\ngo publishMessages(incomingTopic, publisher) ctx := context.Background() if err := router.Run(ctx); err != nil { log.Fatal(err) } func publishMessages(topic string, publisher message.Publisher) { for { msg := message.NewMessage(watermill.NewUUID(), []byte(\u0026#34;Hello, watermill!\u0026#34;)) middleware.SetCorrelationID(watermill.NewUUID(), msg) fmt.Printf(\u0026#34;\\n\\n\\nSending message %s, correlation id: %s\\n\u0026#34;, msg.UUID, middleware.MessageCorrelationID(msg)) if err := publisher.Publish(topic, msg); err != nil { log.Fatal(err) } time.Sleep(3 * time.Second) } } 啟動服務：\ndocker-compose up 若把 WATERMILL_PUBSUB_TYPE 環境變數設為空字串就可以將底層的 broker 換成 GoChannel Pub/Sub，有興趣可以實驗看看，兩者的 output 會是一模一樣的。\nMessaging Systems 前面談到 message broker 可以是各種不同的實作，比如 RabbitMQ 或 Kafka 等。但各種訊息系統有什麼不同呢？這邊會做一個簡單的比較與整理。\n下圖來自我在簡報中做的 Message broker 比較與整理：\n Kafka 是許多企業的 event streaming 平台首選，這是因爲他的高 throughput、擴展性與可靠性。而 RabbitMQ 實作了 AMQP 協定，它豐富的 routing 機制讓我們可以處理很複雜的資料流。而 NATS Streaming 是三者中最年輕的，它是以 Golang 實作的 CNCF 專案，十分輕快速，部署也相對容易，並且結合了前述兩者的優點。\n舉例來說，NATS Streaming 有以下特性：\n 由上圖可以看到，NATS Streaming 結合了 Kafka 的 ConsumerGroup 特點與 RabbitMQ 的路由 matching 機制，讓我們在開發上有更多選擇的彈性。\n總結 事件驅動能夠以貼近真實業務邏輯的方式描述系統架構，並幫助我們解耦服務依賴，提高擴展性，而 Watermill 更使得這一切變的容易實現。\n很高興這次能夠在 Golang Taipei 分享我的一些想法，這次的講者經驗也讓我體會到社群滿滿的熱情與活力！\nReference  https://www.rabbitmq.com http://kafka.apache.org https://nats.io https://www.redhat.com/en/topics/integration/what-is-event-driven-architecture https://arxiv.org/pdf/1912.03715.pdf https://github.com/ThreeDotsLabs/watermill https://watermill.io/docs/cqrs/#building-a-read-modelwith-the-event-handler ","permalink":"https://minghsu.io/posts/event-driven-golang-taipei-55/","tags":["Event Driven","Web","Golang","Watermill"],"title":"輕鬆「Go」建事件驅動應用"},{"categories":["Ops","K8s"],"contents":"這個教學使用 DroneCI 與 ArgoCD 打造 cloud-native 的持續整合交付平台，讓我們在 push commit 或 merge PR 後即可自動跑完測試、打包 image 並部署到 K8s 叢集。必且藉由版控，我們也得以輕鬆 rollback 到之前的任一版本！\nSource code\nDroneCI 介紹 DroneCI 是一個 cloud-native 的 CI (Continuous Integration) 工具。它很好的整合了 Github、Gitlab 與 Bitbucket 等多種程式碼托管平台，讓我們可以直接同步 repository 到 Drone 上。如同 TravisCI 與其他的 CI 工具一樣，我們可以用一個 yaml 檔描述我們的 pipeline (比如 .drone.yml)，而 Drone 在偵測到程式碼異動後就會觸發 webhook 去執行它。\n舉例來說，以下的 .drone.yml 描述了如何跑一個 Golang 應用的測試並發布到 Dockerhub 上：\nkind: pipeline steps: - name: test image: golang commands: - go test - go build - name: publish image: plugins/docker settings: repo: octocat/hello-world tags: [ latest, 1.0, 1 ] Drone 特別的是它的每個 pipeline step 都是一個 container，因此我們可以很大程度的客製化符合自身需求的 pipeline，或是可以很簡單的就啟動測試用的外部服務。而 Drone 官方也提供許多實用的 plugins 供我們使用，比如 drone-docker 可以用來打包 image，而 drone-slack 可以讓我們輕鬆結合 Slack notification。有興趣的話也可以貢獻自己的 plugins，比如這個 Drone 的 Plugin Registry 彙整了眾多社群提供的 Plugins。\nArgoCD 介紹 ArgoCD 幫助我們同步 Git 上的 manitests 與 K8s 叢集資源的狀態。也就是說，我們只需要在版控上維護系統的部署狀態 (如 image 的版本、資源限制的設定等)，ArgoCD 就會自動幫我們同步到機器上，並且確認服務是否健康。另外，我們也可以善用大家最熟悉的版控來管理服務，因此 rollback 到任一版本都是非常容易的。\n而這樣管理 K8s 叢集與應用程式交付的方式就叫做 GitOps。GitOps 讓我們可以維護服務部署狀態的 \u0026ldquo;source of truth\u0026rdquo;，進而提升團隊維護的效率與系統的可靠性。\nOverview  使用者 push 程式碼或是 merge 新的 PR 觸發 webhook，Drone 開始執行定義在 .drone.yml 的 CI pipeline 若測試通過就發布新的 image 到 Dockerhub 上，並更新 manitests repository 上的 image 版本 ArgoCD 偵測到 manifests 的變動，因此通知 K8s 更新 image 並同步部署狀態  事先準備 Source code 可以看 這裡。\n 一個 Drone server  Github installation   一個測試用 K8s 叢集  K3d minikube K0s   部署 ArgoCD 到叢集上  All-in-one installation   一個 Github 帳號與一個 Dockerhub 帳號  DroneCI Setup 當你成功的在 Drone 上連動 Github 帳號後，你可以在 Drone 的 dashboard 上看到所有的 repo。接著複製這個 repo、activate 它並前往 Repositories -\u0026gt; cicd-demo -\u0026gt; settings 新增以下 secret：\n docker_username: 你的 Dockerhub 帳號 docker_password: 你的 Dockerhub 密碼 ssh_key: Github 的 SSH private key  最後修改 .drone.yml，把 minghsu0107 替換成你自己的 Github 與 Dockerhub 帳號。現在在 main branch 上的 push 或 pull request 都會觸發 Drone pipeline，有關 Drone 的 Github webhook 的設定可以前往 your repo -\u0026gt; setting -\u0026gt; webhook 查看。\n本機開發 在本機開發時，我們會想要檢查 .drone.yml 是否撰寫正確但又不想每次都 push 到 repo。此時我們可以使用 Drone CLI。它可以讓我們在本機執行 pipeline，並且可以 include 或 exclude 某幾個 step，非常適合用在開發上。\n使用 CLI 登入 Drone:\nexport DRONE_SERVER=\u0026lt;drone-server-url\u0026gt; export DRONE_TOKEN=\u0026lt;drone-token\u0026gt; # check token under: dashboard -\u0026gt; user setting drone info 舉例來說，我們可以僅執行 test 這個 step：\ndrone exec --include=\u0026lt;pipline-step-name\u0026gt; ArgoCD 開始前請先 clone 這個 manifest repository。這個 repo 負責所有有關部署應用的 manifests，之後會用來與 ArgoCD 同步。這邊使用 Kustomize 這個模板工具維護 K8s 的 resources，而且 Kustomize 是被 ArgoCD 原生支援的，這裡就不細談 Kustomize 的使用方法了，只要知道它在這裡幫助我們區分 dev 與 prod 環境，並提高 manifests 的可讀性即可。\n如果你的 repo 是 private 的，你必須要在 ArgoCD 上設定 credentials，否則就可以跳過這個步驟。\n使用 Argo CD CLI:\nargocd repo add \u0026lt;repo-url\u0026gt; --username=\u0026lt;username\u0026gt; --password=\u0026lt;password\u0026gt; 也可以使用 GUI：前往 Settings/Repositories、點擊 Connect Repo using HTTPS 並輸入 credentials：\n你會看到如以下的畫面：\n新增一個 app：\n記得把 repo 替換成你自己的。\n現在我們完成了所有的準備，前往 /applications 並點擊 SYNC 就可以看到 ArgoCD 自動同步了 cluster 的狀態！\n也可以點進去 app 看看一些詳細資訊：\n總結 DroneCI 與 ArgoCD 都是滿好上手的工具，而且功能也很強大。今後大家若要建置 CI/CD 平台，但又希望自己 host 伺服器而非使用第三方提供的服務 (可能有成本的考量等)，DroneCI 與 ArgoCD 的組合是一個值得考慮的選擇。\nReference  https://www.weave.works/technologies/gitops/ https://argo-cd.readthedocs.io/en/stable/ https://docs.drone.io https://hub.docker.com/r/line/kubectl-kustomize ","permalink":"https://minghsu.io/posts/droneci-argocd/","tags":["K8s","DroneCI","ArgoCD","CI/CD","GitOps"],"title":"[K8s] 使用 DroneCI 與 ArgoCD 實現 K8s 自動整合部署"},{"categories":null,"contents":"I am Hao-Ming Hsu, now studying Computer Science in the National Taiwan University. Site reliability engineering (SRE) and cloud-native application development are main subjects I\u0026rsquo;m focusing on.\nAs a passionate developer, I enjoy learning new things and sharing them with developer communities. Thus, I\u0026rsquo;d like to share my thoughts and some learning notes in this blog. Further dicussion is welcome under each post!\nContact Me  minghsu0107@gmail.com https://linkedin.com/in/hao-ming-hsu-178176181  ","permalink":"https://minghsu.io/about/","tags":null,"title":"About Me"},{"categories":["Conference"],"contents":"以下與大家分享一些這次 JCConf 2020 中我認為值得討論的主題。\n整體來說，滿多議程在推廣 Kotlin 、serverless 與微服務的使用，而大家滿心期待的 Java virtual thread 也已如火如荼的開發中！\n用開源的 MySQL 叢集解決大型網路應用的擴充性問題 現今後端應用的 bottleneck 常常是出現在資料庫上。因此當資料量大到一定程度時，解決資料庫的吞吐瓶頸就會成為一大挑戰。\n常見提高資料庫吞吐量的方式有垂直擴充與水平擴充。垂直擴充是以增加 cpu 與 RAM 等硬體資源為機器升級以提高讀寫效能，而水平擴充則是增加機器並運用分布式架構擴充資料規模，比較常見的方式有主從式讀寫分離架構或資料分片 (sharding) 等。然而對於傳統關聯式資料庫來說，水平擴展比起 NoSQL 困難許多，因為其涉及到了資料的切分或一致性等複雜的問題。\nMySQL 叢集 為了解決上述的痛點，MySQL 提出了最新的高可用方案，包含了 MySQL Replication、MySQL Group Replication、Shared Disk Based Active/Passive 與 MySQL Cluster：\n舉例來說，使用 MySQL Cluster 和 MySQL Group Replication多主模式的話，便可用 Connector/J 這個 Java SQL driver 達成負載平衡。而 MySQL Replicatino 與 MySQL Group Replication 的單主模式下則可用 Connector/J 的複製/讀寫分離模式 (只需將 connection object 設為 read-only)。另外，只需一行便可在 Connector/J 的 JDBC 上完成故障移轉。\n因此，當我們的環境有以下的擴充需求時就可以考慮使用 MySQL 叢集：\n 高可用不停機 寫的高吞吐 高併發連線 高擴展 使用簡單和具有彈性  MySQL \u0026amp; NoSQL Cluster/J 是個另一個 MySQL Cluster 的 driver。它不經 SQL 節點，在 Java程式中透過 JNI 直接呼叫以 C 所開發的NDB API，目的是將 MySQL 以表為主的資料對應到 Java 程式的物件。使用 Cluster/J 便可以 NoSQL 的方式直接取用 MySQL Cluster 上的資料。Cluster/J 以 ClusterJPA 抽象化，支援JPA 而增加可攜性並提供以下特性：\n Persistent classes Relationships Joins in queries Lazy loading Table and index creation from object model  這是在官網上拿來的例子，假設我們現在要新增一位 employee：\nEmployee newEmployee = session.newInstance(Employee.class); session 物件對應一個 cluster/J 與 MySQL 的 connection. 下面的程式碼對應到 insert 操作，有些 ORM 的味道：\nemp.setId(988); newEmployee.setFirstName(\u0026#34;John\u0026#34;); newEmployee.setLastName(\u0026#34;Jones\u0026#34;); newEmployee.setStarted(new Date()); 而這段查詢的程式對應到 SELECT * FROM employee WHERE id = 988：\nEmployee theEmployee = session.find(Employee.class, 988); Cluster/J的增刪改的性能非常好，幾乎和 native NDB API 差不多，查詢為一般JDBC的兩倍快：\n何時使用 由於 Cluster/J 直接訪問 Data Node 可達2億 QPS，而 Connector/J 對 SQL Node 則可到每秒250萬個 SQL 指令。因此推薦複雜的查詢可以透過 SQL 指令查詢，而 Key-Value 的操作則以 Cluster/J 加快操作速度。而這些都是建立在 MySQL 叢集上的，因此兩種方式都可保有高可用與擴充性。\nRSocket RSocket 是個使用 byte stream 的 binary protocol。它支援四種 interaction models：\n request/response (stream of 1) request/stream (finite stream of many) fire-and-forget (no response) channel (bi-directional streams)  由於另一位也有參加會議的 Allen 已詳細解說了與 Spring Boot 整合的使用方式。我在此補充並分享一些 RSocket 的特點還有與 gRPC (HTTP/2) 的比較，讓大家更清楚為何一些大公司如阿里巴巴與 Facebook 會選擇在 production 中使用 RSocket 作為通訊基礎。\n 輕量：RSocket 的四個 interactive model 被規範在 RSocket 協定內，而 gRPC 則是藉由 HTTP/2 Stream 傳輸這些 RPC 的資訊，因此 gRPC 傳輸的 overhead 會比較大。 彈性：RSocket 在資料的傳輸上更具有彈性，它所需要的只是一個 duplex connection，不像 gRPC 需要事先定義好 protobuf 並且只能使用 protobuf 有定義好的 procudure。另外，gRPC 需要 code generation，但 RSocket 提供開發者更多選擇：喜歡 RPC style 的人可以使用 RSocket-RPC，希望與 Spring 整合的話也可以使用 Spring Messaging，當然直接操作 RSocket 也完全沒問題。 Session 複用：RSocket 可以在多次連線之間恢復 long-lived streams。這使的 RSocket 特別適合 mobile 與 server 端的溝通場景。 Back-pressure：RSocket 實作了 reactive streams 並支援異步 back-pressure (消費者需要多少，生產者就生產多少)，這樣的 application-level flow control 是基於 HTTP/2 的 gRPC 無法做到的。 既是 client 也是 server：RSocket 在連線建立後 server 與 client 都能成為 requester 與 responder。換句話說 RSocket 是 fully duplex 的，client 與 server 都能在同個連線下發出和接受請求。相較之下 gRPC 是標準的 client-server 模型，因此 server 端無法向 client 發出請求。 瀏覽器支援：gRPC 在瀏覽器中沒有直接支援，需要額外的函式庫且提高了維護的複雜度。而 RSocket 可以藉由 Websocket 在瀏覽器直接運作，我們只需要開啟一個接受 Websocket 連線的 RSocket instance 即可。  總結來說，RSocket 為 reactive programming 與 event driven 架構打下了可靠的基礎。它為我們帶來嶄新的服務通訊方式，而其輕量的特性也使我們能更輕鬆擴展 real-time 的 application。\n其他議程  街口支付的唯服務之路：分享了街口支付如何將高耦合的單體架構重構成領域服務邊界明確的微服務：  定義業務領域 (Business Domain) - 找出並定義所有商業行為所包含的 domain knowledge，與該領域的專業人討論再進行業務切分。 明確領域服務邊界 - 強調 Conway\u0026rsquo;s Law，系統架構即是公司組織的反映 。 重構與持續優化   快來一起認識 Helm 吧！：介紹了 Helm 這個 Kubernetes 的套件管理工具，並現場使用 k0s + Docker-Compose 實作。詳細內容請看 Github。  總結 第一次參加 JCConf 這個 Java 年度盛會覺得收穫很多。看到了許多很有料的講者，也深刻的體會到了 Java 社群源源不絕的熱忱與活力。中場休息時間在攤位上也聽到許多新鮮有趣的 sharing。熱愛 Java 跟收集貼紙的人絕對不能錯過！\nReference  MySQL Cluster Slides, JCConf 2020 Differences between gRPC and RSocket - by Robert B Roeser  k0s Helm Demo, JCConf 2020 ","permalink":"https://minghsu.io/posts/jcconf-2020/","tags":["JCConf","Java"],"title":"JCConf 2020 心得"},{"categories":["Blogging"],"contents":"用 Hugo + Github Pages + Github Actions 快速架設部落格網站。\nHugo 是一個用 Go 撰寫的開源網站架設引擎。它幫助我們得以快速得架設一個靜態網站。\n安裝 Hugo  https://gohugo.io/getting-started/installing/ https://github.com/gohugoio/hugo/releases/  啟動新 Hugo 專案:\nhugo new site myblog -f yaml 基本設定 初始化 Repo:\ngit init 下載 Hugo 的 theme (使用 git submodule)：\ngit submodule add https://github.com/spf13/hyde themes/hyde 修改 config.yaml:\nbaseURL: https://minghsu0107.github.io/ languageCode: en-us title: Ming\u0026#39;s Site # 記得設定你的 theme theme : \u0026#39;hyde\u0026#39; 新增新的 post:\nhugo new posts/first-post.md 修改 /content/posts/first-post.md:\n--- title: \u0026#34;First Post\u0026#34; date: 2021-02-24T16:27:23+08:00 draft: false --- My first post!!! 啟動 testing server:\nhugo server -w 成功的話會看到你的第一篇 post：\n部署到 Github Pages 在 .github/workflows/gh-pages.yaml 新增打包與部署 hugo 的 Github Actions:\nname: github pages on: push: branches: - main  # Set a branch name to trigger deployment pull_request: jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;0.83.1\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public cname: minghsu.io 接著新增一個新的分支 gh-pages，Github Actions 之後會將部落格打包並部署這個分支：\ngit checkout -b gh-pages git push origin gh-pages 把 code 推到 Github 上之後，Github Actions 會開始部署部落格。不過這次的部署會失敗，因為我們還沒設定 Github Pages 的 branch。因此接著我們要到 Settings -\u0026gt; Pages 將 Github Pages 的分支設定為 gh-pages。現在 Github Pages 會重新開始部署，成功之後瀏覽 https://minghsu.io 就能看到部落格！\nReference  https://themes.gohugo.io/hyde/ https://gohugo.io/templates/lookup-order/ ","permalink":"https://minghsu.io/posts/first-post/","tags":["Hugo","Golang"],"title":"我如何架設這個部落格的？"},{"categories":null,"contents":"Nothing on this page will be visible. This file exists solely to respond to /search URL.\nSetting a very low sitemap priority will tell search engines this is not important content.\n","permalink":"https://minghsu.io/search/","tags":null,"title":"Search Results"}]